{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read text file\n",
    "file = open(\"text.txt\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt = word_tokenize(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'world',\n",
       " 'is',\n",
       " 'changing',\n",
       " 'at',\n",
       " 'an',\n",
       " 'unprecedented',\n",
       " 'pace',\n",
       " ',',\n",
       " 'driven',\n",
       " 'by',\n",
       " 'advances',\n",
       " 'in',\n",
       " 'technology',\n",
       " ',',\n",
       " 'globalization',\n",
       " ',',\n",
       " 'and',\n",
       " 'shifting',\n",
       " 'social',\n",
       " 'attitudes',\n",
       " '.',\n",
       " 'As',\n",
       " 'a',\n",
       " 'result',\n",
       " ',',\n",
       " 'many',\n",
       " 'individuals',\n",
       " 'and',\n",
       " 'organizations',\n",
       " 'are',\n",
       " 'struggling',\n",
       " 'to',\n",
       " 'keep',\n",
       " 'up',\n",
       " 'with',\n",
       " 'the',\n",
       " 'rapid',\n",
       " 'pace',\n",
       " 'of',\n",
       " 'change',\n",
       " ',',\n",
       " 'and',\n",
       " 'are',\n",
       " 'finding',\n",
       " 'themselves',\n",
       " 'left',\n",
       " 'behind',\n",
       " '.',\n",
       " 'In',\n",
       " 'order',\n",
       " 'to',\n",
       " 'thrive',\n",
       " 'in',\n",
       " 'this',\n",
       " 'new',\n",
       " 'world',\n",
       " ',',\n",
       " 'it',\n",
       " 'is',\n",
       " 'essential',\n",
       " 'to',\n",
       " 'embrace',\n",
       " 'a',\n",
       " 'growth',\n",
       " 'mindset',\n",
       " 'and',\n",
       " 'be',\n",
       " 'willing',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'and',\n",
       " 'adapt',\n",
       " '.',\n",
       " 'One',\n",
       " 'of',\n",
       " 'the',\n",
       " 'key',\n",
       " 'components',\n",
       " 'of',\n",
       " 'a',\n",
       " 'growth',\n",
       " 'mindset',\n",
       " 'is',\n",
       " 'a',\n",
       " 'willingness',\n",
       " 'to',\n",
       " 'take',\n",
       " 'risks',\n",
       " 'and',\n",
       " 'embrace',\n",
       " 'failure',\n",
       " 'as',\n",
       " 'a',\n",
       " 'necessary',\n",
       " 'part',\n",
       " 'of',\n",
       " 'the',\n",
       " 'learning',\n",
       " 'process',\n",
       " '.',\n",
       " 'In',\n",
       " 'today',\n",
       " \"'s\",\n",
       " 'world',\n",
       " ',',\n",
       " 'the',\n",
       " 'ability',\n",
       " 'to',\n",
       " 'fail',\n",
       " 'fast',\n",
       " 'and',\n",
       " 'learn',\n",
       " 'quickly',\n",
       " 'is',\n",
       " 'more',\n",
       " 'important',\n",
       " 'than',\n",
       " 'ever',\n",
       " 'before',\n",
       " '.',\n",
       " 'This',\n",
       " 'means',\n",
       " 'being',\n",
       " 'willing',\n",
       " 'to',\n",
       " 'experiment',\n",
       " ',',\n",
       " 'try',\n",
       " 'new',\n",
       " 'things',\n",
       " ',',\n",
       " 'and',\n",
       " 'accept',\n",
       " 'that',\n",
       " 'not',\n",
       " 'every',\n",
       " 'idea',\n",
       " 'will',\n",
       " 'be',\n",
       " 'a',\n",
       " 'success',\n",
       " '.',\n",
       " 'It',\n",
       " 'also',\n",
       " 'means',\n",
       " 'being',\n",
       " 'open',\n",
       " 'to',\n",
       " 'feedback',\n",
       " 'and',\n",
       " 'criticism',\n",
       " ',',\n",
       " 'and',\n",
       " 'using',\n",
       " 'it',\n",
       " 'as',\n",
       " 'an',\n",
       " 'opportunity',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'and',\n",
       " 'improve',\n",
       " '.',\n",
       " 'Another',\n",
       " 'important',\n",
       " 'aspect',\n",
       " 'of',\n",
       " 'a',\n",
       " 'growth',\n",
       " 'mindset',\n",
       " 'is',\n",
       " 'a',\n",
       " 'focus',\n",
       " 'on',\n",
       " 'continuous',\n",
       " 'learning',\n",
       " 'and',\n",
       " 'development',\n",
       " '.',\n",
       " 'In',\n",
       " 'a',\n",
       " 'world',\n",
       " 'where',\n",
       " 'skills',\n",
       " 'become',\n",
       " 'obsolete',\n",
       " 'at',\n",
       " 'an',\n",
       " 'alarming',\n",
       " 'rate',\n",
       " ',',\n",
       " 'it',\n",
       " 'is',\n",
       " 'essential',\n",
       " 'to',\n",
       " 'be',\n",
       " 'constantly',\n",
       " 'learning',\n",
       " 'and',\n",
       " 'acquiring',\n",
       " 'new',\n",
       " 'skills',\n",
       " '.',\n",
       " 'This',\n",
       " 'can',\n",
       " 'be',\n",
       " 'done',\n",
       " 'through',\n",
       " 'a',\n",
       " 'variety',\n",
       " 'of',\n",
       " 'means',\n",
       " ',',\n",
       " 'including',\n",
       " 'formal',\n",
       " 'education',\n",
       " ',',\n",
       " 'online',\n",
       " 'courses',\n",
       " ',',\n",
       " 'mentorship',\n",
       " ',',\n",
       " 'and',\n",
       " 'self-directed',\n",
       " 'learning',\n",
       " '.',\n",
       " 'By',\n",
       " 'making',\n",
       " 'a',\n",
       " 'commitment',\n",
       " 'to',\n",
       " 'lifelong',\n",
       " 'learning',\n",
       " ',',\n",
       " 'individuals',\n",
       " 'can',\n",
       " 'ensure',\n",
       " 'that',\n",
       " 'they',\n",
       " 'remain',\n",
       " 'relevant',\n",
       " 'and',\n",
       " 'adaptable',\n",
       " 'in',\n",
       " 'an',\n",
       " 'ever-changing',\n",
       " 'world',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop__words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop__words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the words that are not in the stop words list\n",
    "filtered__words = [w for w in wt if not w in stop__words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'world',\n",
       " 'changing',\n",
       " 'unprecedented',\n",
       " 'pace',\n",
       " ',',\n",
       " 'driven',\n",
       " 'advances',\n",
       " 'technology',\n",
       " ',',\n",
       " 'globalization',\n",
       " ',',\n",
       " 'shifting',\n",
       " 'social',\n",
       " 'attitudes',\n",
       " '.',\n",
       " 'As',\n",
       " 'result',\n",
       " ',',\n",
       " 'many',\n",
       " 'individuals',\n",
       " 'organizations',\n",
       " 'struggling',\n",
       " 'keep',\n",
       " 'rapid',\n",
       " 'pace',\n",
       " 'change',\n",
       " ',',\n",
       " 'finding',\n",
       " 'left',\n",
       " 'behind',\n",
       " '.',\n",
       " 'In',\n",
       " 'order',\n",
       " 'thrive',\n",
       " 'new',\n",
       " 'world',\n",
       " ',',\n",
       " 'essential',\n",
       " 'embrace',\n",
       " 'growth',\n",
       " 'mindset',\n",
       " 'willing',\n",
       " 'learn',\n",
       " 'adapt',\n",
       " '.',\n",
       " 'One',\n",
       " 'key',\n",
       " 'components',\n",
       " 'growth',\n",
       " 'mindset',\n",
       " 'willingness',\n",
       " 'take',\n",
       " 'risks',\n",
       " 'embrace',\n",
       " 'failure',\n",
       " 'necessary',\n",
       " 'part',\n",
       " 'learning',\n",
       " 'process',\n",
       " '.',\n",
       " 'In',\n",
       " 'today',\n",
       " \"'s\",\n",
       " 'world',\n",
       " ',',\n",
       " 'ability',\n",
       " 'fail',\n",
       " 'fast',\n",
       " 'learn',\n",
       " 'quickly',\n",
       " 'important',\n",
       " 'ever',\n",
       " '.',\n",
       " 'This',\n",
       " 'means',\n",
       " 'willing',\n",
       " 'experiment',\n",
       " ',',\n",
       " 'try',\n",
       " 'new',\n",
       " 'things',\n",
       " ',',\n",
       " 'accept',\n",
       " 'every',\n",
       " 'idea',\n",
       " 'success',\n",
       " '.',\n",
       " 'It',\n",
       " 'also',\n",
       " 'means',\n",
       " 'open',\n",
       " 'feedback',\n",
       " 'criticism',\n",
       " ',',\n",
       " 'using',\n",
       " 'opportunity',\n",
       " 'learn',\n",
       " 'improve',\n",
       " '.',\n",
       " 'Another',\n",
       " 'important',\n",
       " 'aspect',\n",
       " 'growth',\n",
       " 'mindset',\n",
       " 'focus',\n",
       " 'continuous',\n",
       " 'learning',\n",
       " 'development',\n",
       " '.',\n",
       " 'In',\n",
       " 'world',\n",
       " 'skills',\n",
       " 'become',\n",
       " 'obsolete',\n",
       " 'alarming',\n",
       " 'rate',\n",
       " ',',\n",
       " 'essential',\n",
       " 'constantly',\n",
       " 'learning',\n",
       " 'acquiring',\n",
       " 'new',\n",
       " 'skills',\n",
       " '.',\n",
       " 'This',\n",
       " 'done',\n",
       " 'variety',\n",
       " 'means',\n",
       " ',',\n",
       " 'including',\n",
       " 'formal',\n",
       " 'education',\n",
       " ',',\n",
       " 'online',\n",
       " 'courses',\n",
       " ',',\n",
       " 'mentorship',\n",
       " ',',\n",
       " 'self-directed',\n",
       " 'learning',\n",
       " '.',\n",
       " 'By',\n",
       " 'making',\n",
       " 'commitment',\n",
       " 'lifelong',\n",
       " 'learning',\n",
       " ',',\n",
       " 'individuals',\n",
       " 'ensure',\n",
       " 'remain',\n",
       " 'relevant',\n",
       " 'adaptable',\n",
       " 'ever-changing',\n",
       " 'world',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered__words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "postag = pos_tag(filtered__words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('world', 'NN'),\n",
       " ('changing', 'VBG'),\n",
       " ('unprecedented', 'JJ'),\n",
       " ('pace', 'NN'),\n",
       " (',', ','),\n",
       " ('driven', 'JJ'),\n",
       " ('advances', 'NNS'),\n",
       " ('technology', 'NN'),\n",
       " (',', ','),\n",
       " ('globalization', 'NN'),\n",
       " (',', ','),\n",
       " ('shifting', 'VBG'),\n",
       " ('social', 'JJ'),\n",
       " ('attitudes', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('As', 'IN'),\n",
       " ('result', 'NN'),\n",
       " (',', ','),\n",
       " ('many', 'JJ'),\n",
       " ('individuals', 'NNS'),\n",
       " ('organizations', 'NNS'),\n",
       " ('struggling', 'VBG'),\n",
       " ('keep', 'VB'),\n",
       " ('rapid', 'JJ'),\n",
       " ('pace', 'NN'),\n",
       " ('change', 'NN'),\n",
       " (',', ','),\n",
       " ('finding', 'VBG'),\n",
       " ('left', 'VBD'),\n",
       " ('behind', 'NN'),\n",
       " ('.', '.'),\n",
       " ('In', 'IN'),\n",
       " ('order', 'NN'),\n",
       " ('thrive', 'JJ'),\n",
       " ('new', 'JJ'),\n",
       " ('world', 'NN'),\n",
       " (',', ','),\n",
       " ('essential', 'JJ'),\n",
       " ('embrace', 'NN'),\n",
       " ('growth', 'NN'),\n",
       " ('mindset', 'VBD'),\n",
       " ('willing', 'JJ'),\n",
       " ('learn', 'JJ'),\n",
       " ('adapt', 'NN'),\n",
       " ('.', '.'),\n",
       " ('One', 'CD'),\n",
       " ('key', 'JJ'),\n",
       " ('components', 'NNS'),\n",
       " ('growth', 'NN'),\n",
       " ('mindset', 'VBP'),\n",
       " ('willingness', 'NN'),\n",
       " ('take', 'NN'),\n",
       " ('risks', 'NNS'),\n",
       " ('embrace', 'VBP'),\n",
       " ('failure', 'NN'),\n",
       " ('necessary', 'JJ'),\n",
       " ('part', 'NN'),\n",
       " ('learning', 'VBG'),\n",
       " ('process', 'NN'),\n",
       " ('.', '.'),\n",
       " ('In', 'IN'),\n",
       " ('today', 'NN'),\n",
       " (\"'s\", 'POS'),\n",
       " ('world', 'NN'),\n",
       " (',', ','),\n",
       " ('ability', 'NN'),\n",
       " ('fail', 'VBP'),\n",
       " ('fast', 'NN'),\n",
       " ('learn', 'JJ'),\n",
       " ('quickly', 'RB'),\n",
       " ('important', 'JJ'),\n",
       " ('ever', 'RB'),\n",
       " ('.', '.'),\n",
       " ('This', 'DT'),\n",
       " ('means', 'VBZ'),\n",
       " ('willing', 'JJ'),\n",
       " ('experiment', 'NN'),\n",
       " (',', ','),\n",
       " ('try', 'VB'),\n",
       " ('new', 'JJ'),\n",
       " ('things', 'NNS'),\n",
       " (',', ','),\n",
       " ('accept', 'VBP'),\n",
       " ('every', 'DT'),\n",
       " ('idea', 'NN'),\n",
       " ('success', 'NN'),\n",
       " ('.', '.'),\n",
       " ('It', 'PRP'),\n",
       " ('also', 'RB'),\n",
       " ('means', 'VBZ'),\n",
       " ('open', 'JJ'),\n",
       " ('feedback', 'NN'),\n",
       " ('criticism', 'NN'),\n",
       " (',', ','),\n",
       " ('using', 'VBG'),\n",
       " ('opportunity', 'NN'),\n",
       " ('learn', 'NN'),\n",
       " ('improve', 'VB'),\n",
       " ('.', '.'),\n",
       " ('Another', 'DT'),\n",
       " ('important', 'JJ'),\n",
       " ('aspect', 'NN'),\n",
       " ('growth', 'NN'),\n",
       " ('mindset', 'VBP'),\n",
       " ('focus', 'NN'),\n",
       " ('continuous', 'JJ'),\n",
       " ('learning', 'NN'),\n",
       " ('development', 'NN'),\n",
       " ('.', '.'),\n",
       " ('In', 'IN'),\n",
       " ('world', 'NN'),\n",
       " ('skills', 'NNS'),\n",
       " ('become', 'VBP'),\n",
       " ('obsolete', 'JJ'),\n",
       " ('alarming', 'VBG'),\n",
       " ('rate', 'NN'),\n",
       " (',', ','),\n",
       " ('essential', 'JJ'),\n",
       " ('constantly', 'RB'),\n",
       " ('learning', 'VBG'),\n",
       " ('acquiring', 'VBG'),\n",
       " ('new', 'JJ'),\n",
       " ('skills', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('This', 'DT'),\n",
       " ('done', 'VBN'),\n",
       " ('variety', 'NN'),\n",
       " ('means', 'NNS'),\n",
       " (',', ','),\n",
       " ('including', 'VBG'),\n",
       " ('formal', 'JJ'),\n",
       " ('education', 'NN'),\n",
       " (',', ','),\n",
       " ('online', 'NN'),\n",
       " ('courses', 'NNS'),\n",
       " (',', ','),\n",
       " ('mentorship', 'NN'),\n",
       " (',', ','),\n",
       " ('self-directed', 'JJ'),\n",
       " ('learning', 'NN'),\n",
       " ('.', '.'),\n",
       " ('By', 'IN'),\n",
       " ('making', 'VBG'),\n",
       " ('commitment', 'NN'),\n",
       " ('lifelong', 'RB'),\n",
       " ('learning', 'NN'),\n",
       " (',', ','),\n",
       " ('individuals', 'NNS'),\n",
       " ('ensure', 'VB'),\n",
       " ('remain', 'VBP'),\n",
       " ('relevant', 'JJ'),\n",
       " ('adaptable', 'JJ'),\n",
       " ('ever-changing', 'JJ'),\n",
       " ('world', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "stemList = []\n",
    "for words in filtered__words:\n",
    "    stemList.append([words, ps.stem(words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'the'],\n",
       " ['world', 'world'],\n",
       " ['changing', 'chang'],\n",
       " ['unprecedented', 'unpreced'],\n",
       " ['pace', 'pace'],\n",
       " [',', ','],\n",
       " ['driven', 'driven'],\n",
       " ['advances', 'advanc'],\n",
       " ['technology', 'technolog'],\n",
       " [',', ','],\n",
       " ['globalization', 'global'],\n",
       " [',', ','],\n",
       " ['shifting', 'shift'],\n",
       " ['social', 'social'],\n",
       " ['attitudes', 'attitud'],\n",
       " ['.', '.'],\n",
       " ['As', 'as'],\n",
       " ['result', 'result'],\n",
       " [',', ','],\n",
       " ['many', 'mani'],\n",
       " ['individuals', 'individu'],\n",
       " ['organizations', 'organ'],\n",
       " ['struggling', 'struggl'],\n",
       " ['keep', 'keep'],\n",
       " ['rapid', 'rapid'],\n",
       " ['pace', 'pace'],\n",
       " ['change', 'chang'],\n",
       " [',', ','],\n",
       " ['finding', 'find'],\n",
       " ['left', 'left'],\n",
       " ['behind', 'behind'],\n",
       " ['.', '.'],\n",
       " ['In', 'in'],\n",
       " ['order', 'order'],\n",
       " ['thrive', 'thrive'],\n",
       " ['new', 'new'],\n",
       " ['world', 'world'],\n",
       " [',', ','],\n",
       " ['essential', 'essenti'],\n",
       " ['embrace', 'embrac'],\n",
       " ['growth', 'growth'],\n",
       " ['mindset', 'mindset'],\n",
       " ['willing', 'will'],\n",
       " ['learn', 'learn'],\n",
       " ['adapt', 'adapt'],\n",
       " ['.', '.'],\n",
       " ['One', 'one'],\n",
       " ['key', 'key'],\n",
       " ['components', 'compon'],\n",
       " ['growth', 'growth'],\n",
       " ['mindset', 'mindset'],\n",
       " ['willingness', 'willing'],\n",
       " ['take', 'take'],\n",
       " ['risks', 'risk'],\n",
       " ['embrace', 'embrac'],\n",
       " ['failure', 'failur'],\n",
       " ['necessary', 'necessari'],\n",
       " ['part', 'part'],\n",
       " ['learning', 'learn'],\n",
       " ['process', 'process'],\n",
       " ['.', '.'],\n",
       " ['In', 'in'],\n",
       " ['today', 'today'],\n",
       " [\"'s\", \"'s\"],\n",
       " ['world', 'world'],\n",
       " [',', ','],\n",
       " ['ability', 'abil'],\n",
       " ['fail', 'fail'],\n",
       " ['fast', 'fast'],\n",
       " ['learn', 'learn'],\n",
       " ['quickly', 'quickli'],\n",
       " ['important', 'import'],\n",
       " ['ever', 'ever'],\n",
       " ['.', '.'],\n",
       " ['This', 'thi'],\n",
       " ['means', 'mean'],\n",
       " ['willing', 'will'],\n",
       " ['experiment', 'experi'],\n",
       " [',', ','],\n",
       " ['try', 'tri'],\n",
       " ['new', 'new'],\n",
       " ['things', 'thing'],\n",
       " [',', ','],\n",
       " ['accept', 'accept'],\n",
       " ['every', 'everi'],\n",
       " ['idea', 'idea'],\n",
       " ['success', 'success'],\n",
       " ['.', '.'],\n",
       " ['It', 'it'],\n",
       " ['also', 'also'],\n",
       " ['means', 'mean'],\n",
       " ['open', 'open'],\n",
       " ['feedback', 'feedback'],\n",
       " ['criticism', 'critic'],\n",
       " [',', ','],\n",
       " ['using', 'use'],\n",
       " ['opportunity', 'opportun'],\n",
       " ['learn', 'learn'],\n",
       " ['improve', 'improv'],\n",
       " ['.', '.'],\n",
       " ['Another', 'anoth'],\n",
       " ['important', 'import'],\n",
       " ['aspect', 'aspect'],\n",
       " ['growth', 'growth'],\n",
       " ['mindset', 'mindset'],\n",
       " ['focus', 'focu'],\n",
       " ['continuous', 'continu'],\n",
       " ['learning', 'learn'],\n",
       " ['development', 'develop'],\n",
       " ['.', '.'],\n",
       " ['In', 'in'],\n",
       " ['world', 'world'],\n",
       " ['skills', 'skill'],\n",
       " ['become', 'becom'],\n",
       " ['obsolete', 'obsolet'],\n",
       " ['alarming', 'alarm'],\n",
       " ['rate', 'rate'],\n",
       " [',', ','],\n",
       " ['essential', 'essenti'],\n",
       " ['constantly', 'constantli'],\n",
       " ['learning', 'learn'],\n",
       " ['acquiring', 'acquir'],\n",
       " ['new', 'new'],\n",
       " ['skills', 'skill'],\n",
       " ['.', '.'],\n",
       " ['This', 'thi'],\n",
       " ['done', 'done'],\n",
       " ['variety', 'varieti'],\n",
       " ['means', 'mean'],\n",
       " [',', ','],\n",
       " ['including', 'includ'],\n",
       " ['formal', 'formal'],\n",
       " ['education', 'educ'],\n",
       " [',', ','],\n",
       " ['online', 'onlin'],\n",
       " ['courses', 'cours'],\n",
       " [',', ','],\n",
       " ['mentorship', 'mentorship'],\n",
       " [',', ','],\n",
       " ['self-directed', 'self-direct'],\n",
       " ['learning', 'learn'],\n",
       " ['.', '.'],\n",
       " ['By', 'by'],\n",
       " ['making', 'make'],\n",
       " ['commitment', 'commit'],\n",
       " ['lifelong', 'lifelong'],\n",
       " ['learning', 'learn'],\n",
       " [',', ','],\n",
       " ['individuals', 'individu'],\n",
       " ['ensure', 'ensur'],\n",
       " ['remain', 'remain'],\n",
       " ['relevant', 'relev'],\n",
       " ['adaptable', 'adapt'],\n",
       " ['ever-changing', 'ever-chang'],\n",
       " ['world', 'world'],\n",
       " ['.', '.']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ADMIN/nltk_data'\n    - 'c:\\\\ProgramData\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\ProgramData\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\ProgramData\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ADMIN\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mzip_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     85\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4.zip/omw-1.4/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ADMIN/nltk_data'\n    - 'c:\\\\ProgramData\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\ProgramData\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\ProgramData\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ADMIN\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m lemList \u001b[39m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m words \u001b[39min\u001b[39;00m filtered__words:\n\u001b[1;32m----> 5\u001b[0m     lemList\u001b[39m.\u001b[39mappend([words, wl\u001b[39m.\u001b[39;49mlemmatize(words)])\n\u001b[0;32m      7\u001b[0m lemList\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\nltk\\stem\\wordnet.py:45\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlemmatize\u001b[39m(\u001b[39mself\u001b[39m, word: \u001b[39mstr\u001b[39m, pos: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mn\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m     34\u001b[0m     \u001b[39m\"\"\"Lemmatize `word` using WordNet's built-in morphy function.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39m    :return: The lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     lemmas \u001b[39m=\u001b[39m wn\u001b[39m.\u001b[39;49m_morphy(word, pos)\n\u001b[0;32m     46\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmin\u001b[39m(lemmas, key\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m) \u001b[39mif\u001b[39;00m lemmas \u001b[39melse\u001b[39;00m word\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLazyCorpusLoader object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load()\n\u001b[0;32m    122\u001b[0m \u001b[39m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:89\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[39m# Load the corpus.\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m corpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__reader_cls(root, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kwargs)\n\u001b[0;32m     91\u001b[0m \u001b[39m# This is where the magic happens!  Transform ourselves into\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[39m# the corpus by modifying our own __dict__ and __class__ to\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[39m# match that of the corpus.\u001b[39;00m\n\u001b[0;32m     95\u001b[0m args, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kwargs\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1176\u001b[0m, in \u001b[0;36mWordNetCorpusReader.__init__\u001b[1;34m(self, root, omw_reader)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1173\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe multilingual functions are not available with this Wordnet version\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1174\u001b[0m     )\n\u001b[0;32m   1175\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprovenances \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49momw_prov()\n\u001b[0;32m   1178\u001b[0m \u001b[39m# A cache to store the wordnet data of multiple languages\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lang_data \u001b[39m=\u001b[39m defaultdict(\u001b[39mlist\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1285\u001b[0m, in \u001b[0;36mWordNetCorpusReader.omw_prov\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1283\u001b[0m provdict \u001b[39m=\u001b[39m {}\n\u001b[0;32m   1284\u001b[0m provdict[\u001b[39m\"\u001b[39m\u001b[39meng\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1285\u001b[0m fileids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_omw_reader\u001b[39m.\u001b[39;49mfileids()\n\u001b[0;32m   1286\u001b[0m \u001b[39mfor\u001b[39;00m fileid \u001b[39min\u001b[39;00m fileids:\n\u001b[0;32m   1287\u001b[0m     prov, langfile \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39msplit(fileid)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLazyCorpusLoader object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load()\n\u001b[0;32m    122\u001b[0m \u001b[39m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfind(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubdir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mzip_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[39m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__reader_cls(root, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     82\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ADMIN/nltk_data'\n    - 'c:\\\\ProgramData\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\ProgramData\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\ProgramData\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ADMIN\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "wl = WordNetLemmatizer()\n",
    "\n",
    "lemList = []\n",
    "for words in filtered__words:\n",
    "    lemList.append([words, wl.lemmatize(words)])\n",
    "\n",
    "lemList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 1, 'world': 5, 'changing': 1, 'unprecedented': 1, 'pace': 2, ',': 16, 'driven': 1, 'advances': 1, 'technology': 1, 'globalization': 1, 'shifting': 1, 'social': 1, 'attitudes': 1, '.': 11, 'As': 1, 'result': 1, 'many': 1, 'individuals': 2, 'organizations': 1, 'struggling': 1, 'keep': 1, 'rapid': 1, 'change': 1, 'finding': 1, 'left': 1, 'behind': 1, 'In': 3, 'order': 1, 'thrive': 1, 'new': 3, 'essential': 2, 'embrace': 2, 'growth': 3, 'mindset': 3, 'willing': 2, 'learn': 3, 'adapt': 1, 'One': 1, 'key': 1, 'components': 1, 'willingness': 1, 'take': 1, 'risks': 1, 'failure': 1, 'necessary': 1, 'part': 1, 'learning': 5, 'process': 1, 'today': 1, \"'s\": 1, 'ability': 1, 'fail': 1, 'fast': 1, 'quickly': 1, 'important': 2, 'ever': 1, 'This': 2, 'means': 3, 'experiment': 1, 'try': 1, 'things': 1, 'accept': 1, 'every': 1, 'idea': 1, 'success': 1, 'It': 1, 'also': 1, 'open': 1, 'feedback': 1, 'criticism': 1, 'using': 1, 'opportunity': 1, 'improve': 1, 'Another': 1, 'aspect': 1, 'focus': 1, 'continuous': 1, 'development': 1, 'skills': 2, 'become': 1, 'obsolete': 1, 'alarming': 1, 'rate': 1, 'constantly': 1, 'acquiring': 1, 'done': 1, 'variety': 1, 'including': 1, 'formal': 1, 'education': 1, 'online': 1, 'courses': 1, 'mentorship': 1, 'self-directed': 1, 'By': 1, 'making': 1, 'commitment': 1, 'lifelong': 1, 'ensure': 1, 'remain': 1, 'relevant': 1, 'adaptable': 1, 'ever-changing': 1}\n"
     ]
    }
   ],
   "source": [
    "fre = dict()\n",
    "\n",
    "for words in filtered__words:\n",
    "    if words in fre:\n",
    "        fre[words] += 1\n",
    "    else:\n",
    "        fre[words] = 1\n",
    "\n",
    "print(fre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'The': 1,\n",
       " 'world': 5,\n",
       " 'changing': 1,\n",
       " 'unprecedented': 1,\n",
       " 'pace': 2,\n",
       " ',': 16,\n",
       " 'driven': 1,\n",
       " 'advances': 1,\n",
       " 'technology': 1,\n",
       " 'globalization': 1,\n",
       " 'shifting': 1,\n",
       " 'social': 1,\n",
       " 'attitudes': 1,\n",
       " '.': 11,\n",
       " 'As': 1,\n",
       " 'result': 1,\n",
       " 'many': 1,\n",
       " 'individuals': 2,\n",
       " 'organizations': 1,\n",
       " 'struggling': 1,\n",
       " 'keep': 1,\n",
       " 'rapid': 1,\n",
       " 'change': 1,\n",
       " 'finding': 1,\n",
       " 'left': 1,\n",
       " 'behind': 1,\n",
       " 'In': 3,\n",
       " 'order': 1,\n",
       " 'thrive': 1,\n",
       " 'new': 3,\n",
       " 'essential': 2,\n",
       " 'embrace': 2,\n",
       " 'growth': 3,\n",
       " 'mindset': 3,\n",
       " 'willing': 2,\n",
       " 'learn': 3,\n",
       " 'adapt': 1,\n",
       " 'One': 1,\n",
       " 'key': 1,\n",
       " 'components': 1,\n",
       " 'willingness': 1,\n",
       " 'take': 1,\n",
       " 'risks': 1,\n",
       " 'failure': 1,\n",
       " 'necessary': 1,\n",
       " 'part': 1,\n",
       " 'learning': 5,\n",
       " 'process': 1,\n",
       " 'today': 1,\n",
       " \"'s\": 1,\n",
       " 'ability': 1,\n",
       " 'fail': 1,\n",
       " 'fast': 1,\n",
       " 'quickly': 1,\n",
       " 'important': 2,\n",
       " 'ever': 1,\n",
       " 'This': 2,\n",
       " 'means': 3,\n",
       " 'experiment': 1,\n",
       " 'try': 1,\n",
       " 'things': 1,\n",
       " 'accept': 1,\n",
       " 'every': 1,\n",
       " 'idea': 1,\n",
       " 'success': 1,\n",
       " 'It': 1,\n",
       " 'also': 1,\n",
       " 'open': 1,\n",
       " 'feedback': 1,\n",
       " 'criticism': 1,\n",
       " 'using': 1,\n",
       " 'opportunity': 1,\n",
       " 'improve': 1,\n",
       " 'Another': 1,\n",
       " 'aspect': 1,\n",
       " 'focus': 1,\n",
       " 'continuous': 1,\n",
       " 'development': 1,\n",
       " 'skills': 2,\n",
       " 'become': 1,\n",
       " 'obsolete': 1,\n",
       " 'alarming': 1,\n",
       " 'rate': 1,\n",
       " 'constantly': 1,\n",
       " 'acquiring': 1,\n",
       " 'done': 1,\n",
       " 'variety': 1,\n",
       " 'including': 1,\n",
       " 'formal': 1,\n",
       " 'education': 1,\n",
       " 'online': 1,\n",
       " 'courses': 1,\n",
       " 'mentorship': 1,\n",
       " 'self-directed': 1,\n",
       " 'By': 1,\n",
       " 'making': 1,\n",
       " 'commitment': 1,\n",
       " 'lifelong': 1,\n",
       " 'ensure': 1,\n",
       " 'remain': 1,\n",
       " 'relevant': 1,\n",
       " 'adaptable': 1,\n",
       " 'ever-changing': 1}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words = len(filtered__words)\n",
    "tf = {}\n",
    "\n",
    "for word in filtered__words:\n",
    "    if word in tf:\n",
    "        tf[word] += 1\n",
    "    else:\n",
    "        tf[word] = 1\n",
    "\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "idf = {}\n",
    "\n",
    "num_docs = len(file.read())\n",
    "for word in filtered__words:\n",
    "    count = sum(1 for doc in file.read() if word in doc)\n",
    "    idf[word] = math.log(num_docs / count) if count > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'The': 0,\n",
       " 'world': 0,\n",
       " 'changing': 0,\n",
       " 'unprecedented': 0,\n",
       " 'pace': 0,\n",
       " ',': 0,\n",
       " 'driven': 0,\n",
       " 'advances': 0,\n",
       " 'technology': 0,\n",
       " 'globalization': 0,\n",
       " 'shifting': 0,\n",
       " 'social': 0,\n",
       " 'attitudes': 0,\n",
       " '.': 0,\n",
       " 'As': 0,\n",
       " 'result': 0,\n",
       " 'many': 0,\n",
       " 'individuals': 0,\n",
       " 'organizations': 0,\n",
       " 'struggling': 0,\n",
       " 'keep': 0,\n",
       " 'rapid': 0,\n",
       " 'change': 0,\n",
       " 'finding': 0,\n",
       " 'left': 0,\n",
       " 'behind': 0,\n",
       " 'In': 0,\n",
       " 'order': 0,\n",
       " 'thrive': 0,\n",
       " 'new': 0,\n",
       " 'essential': 0,\n",
       " 'embrace': 0,\n",
       " 'growth': 0,\n",
       " 'mindset': 0,\n",
       " 'willing': 0,\n",
       " 'learn': 0,\n",
       " 'adapt': 0,\n",
       " 'One': 0,\n",
       " 'key': 0,\n",
       " 'components': 0,\n",
       " 'willingness': 0,\n",
       " 'take': 0,\n",
       " 'risks': 0,\n",
       " 'failure': 0,\n",
       " 'necessary': 0,\n",
       " 'part': 0,\n",
       " 'learning': 0,\n",
       " 'process': 0,\n",
       " 'today': 0,\n",
       " \"'s\": 0,\n",
       " 'ability': 0,\n",
       " 'fail': 0,\n",
       " 'fast': 0,\n",
       " 'quickly': 0,\n",
       " 'important': 0,\n",
       " 'ever': 0,\n",
       " 'This': 0,\n",
       " 'means': 0,\n",
       " 'experiment': 0,\n",
       " 'try': 0,\n",
       " 'things': 0,\n",
       " 'accept': 0,\n",
       " 'every': 0,\n",
       " 'idea': 0,\n",
       " 'success': 0,\n",
       " 'It': 0,\n",
       " 'also': 0,\n",
       " 'open': 0,\n",
       " 'feedback': 0,\n",
       " 'criticism': 0,\n",
       " 'using': 0,\n",
       " 'opportunity': 0,\n",
       " 'improve': 0,\n",
       " 'Another': 0,\n",
       " 'aspect': 0,\n",
       " 'focus': 0,\n",
       " 'continuous': 0,\n",
       " 'development': 0,\n",
       " 'skills': 0,\n",
       " 'become': 0,\n",
       " 'obsolete': 0,\n",
       " 'alarming': 0,\n",
       " 'rate': 0,\n",
       " 'constantly': 0,\n",
       " 'acquiring': 0,\n",
       " 'done': 0,\n",
       " 'variety': 0,\n",
       " 'including': 0,\n",
       " 'formal': 0,\n",
       " 'education': 0,\n",
       " 'online': 0,\n",
       " 'courses': 0,\n",
       " 'mentorship': 0,\n",
       " 'self-directed': 0,\n",
       " 'By': 0,\n",
       " 'making': 0,\n",
       " 'commitment': 0,\n",
       " 'lifelong': 0,\n",
       " 'ensure': 0,\n",
       " 'remain': 0,\n",
       " 'relevant': 0,\n",
       " 'adaptable': 0,\n",
       " 'ever-changing': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
